### Adversarial Sparse Transformer for Time Series Forecasting阅读笔记
基于稀疏注意力机制的时间序列预测Transformer


模型提出的原因是因为传统的方法无法解决大规模的时间序列预测问题，绝大多数模型都只优化某一个特定的目标，而现实中所有时间序列是有一些随机倾向的，所以只优化单个目标是不合适的。另一个问题是误差积累，大多数自回归模型都是用生成的值作为输入，而训练的时候是使用真实值作为输入的，这就导致预测的误差积累。


本文提出的模型是一种改进Transformer和GANs相结合的框架，通过鉴别器可以对改进过的Transformer进行正则化，使得其可以学习到更好的时间序列，并且消除累计误差。由于只有少数的历史数据对预测有帮助，所以考虑使用稀疏归一化函数Sparse来进行注意力的归一化处理。

本文设定求解问题为区间预测，区域预测在业务决策和风险管理等方面有着广泛的应用。使用模型估计特定分位数的值是区间预测最直接的方法。因此，论文在每个时间步中输出第50、90个百分位数。$X$为输出对应的时间序列,$Y$为需要预测的值,$\Phi$为需要学习的模型数据。具体定义问题为：
$$
\hat{Y}_{\rho,t_0+1:t_0+\tau} = f_{\rho}(Y_{1:t_0},X_{1:t_0+\tau};\Phi)
$$
通过前面$t_0$个数据，预测后续$\tau$个数据区间的$\rho$分位数。


通过使用基于编码器-解码器的Transformer来预测，因为多头注意层中的注意机制使变压器能够捕获时间序列的长期依赖性。编码器和解码器都由N个相同的层组成。每一层包括两个主要组成部分:多头自注意层和前馈网络。


